<div align="center">

# VLA^2: Vision-Language-Action Agent

[![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-orange.svg)](https://pytorch.org/)
[![VLA](https://img.shields.io/badge/VLA-Vision--Language--Action-green.svg)]()
[![Agent](https://img.shields.io/badge/Agent-Robotics-red.svg)]()
[![LIBERO](https://img.shields.io/badge/LIBERO-Environment-purple.svg)]()
[![License](https://img.shields.io/badge/License-Apache%202.0-yellow.svg)](LICENSE)

</div>

## ğŸ“„ Paper & Resources
- ğŸ“ **Paper**: https://arxiv.org/abs/2510.14902
- ğŸŒ **Project Page**: https://vla-2.github.io

## ğŸ“ Project Structure

```
VLA-2/
â”œâ”€â”€ experiments/                    # Main experimental codes
â”‚   â”œâ”€â”€ robot/                    # Core VLA-2 implementation
â”‚   â”‚   â”œâ”€â”€ openvla_utils.py      # OpenVLA utility functions
â”‚   â”‚   â”œâ”€â”€ robot_utils.py        # Robot interaction utilities
â”‚   â”‚   â””â”€â”€ libero_run/           # Main scripts for LIBERO environment
â”‚   â”‚       â”œâ”€â”€ main_agent_clean.py        # ğŸ¯ Main execution script, use client to get service from vision_planner_service
â”‚   â”‚       â”œâ”€â”€ vision_planner_service.py  # Vision & planning service
â”‚   â”‚       â”œâ”€â”€ qwenvl.py                  # Verification module wrapper
â”‚   â”‚       â”œâ”€â”€ libero_utils.py            # LIBERO environment utilities
â”‚   â”‚       â”œâ”€â”€ regenerate_libero_dataset.py  # Dataset regeneration
â”‚   â”‚       â”œâ”€â”€ mps_start.sh               # Multi-process service start
â”‚   â”‚       â””â”€â”€ mps_stop.sh                # Multi-process service stop
â”‚   â””â”€â”€ val_zsh/                  # Validation shell scripts
â”‚       â”œâ”€â”€ 0.sh, 10.sh           # 0 and 10 test scenarios
â”‚       â”œâ”€â”€ goal.sh, goal_new.sh  # Goal-based evaluations
â”‚       â”œâ”€â”€ objects.sh            # Object manipulation tests
â”‚       â”œâ”€â”€ orange.sh             # Specific object tests
â”‚       â””â”€â”€ spatial.sh            # Spatial reasoning tests
â”œâ”€â”€ script/                       # Tool and utility scripts
â”‚   â”œâ”€â”€ __init__.py              # Package initialization
â”‚   â”œâ”€â”€ auto_DL.py              # Automatic searching utilities
â”‚   â”œâ”€â”€ color.json              # Color configuration
â”‚   â”œâ”€â”€ Judge_simple.py         # Simple judgment module
â”‚   â”œâ”€â”€ mmgdino.py              # MM-GroundingDINO integration, including Vision and Language understanding
â”‚   â”œâ”€â”€ mmgdino_simple.py       # Simplified MM-GroundingDINO
â”‚   â”œâ”€â”€ qwenvl_meg.py           # QwenVL model enhancement
â”‚   â”œâ”€â”€ SAM2_1.py               # Segment Anything Model 2.1
â”‚   â”œâ”€â”€ SAPdivision.py          # SAP (Sub-Action Planning) division
â”‚   â”œâ”€â”€ segvideo.py             # Video segmentation
â”‚   â”œâ”€â”€ segvideo_simple.py      # Simplified video segmentation
â”‚   â”œâ”€â”€ Wholebody.py            # A media function
â”‚   â””â”€â”€ test_images/            # Test images and configurations
â”‚       â”œâ”€â”€ info.json           # Image metadata
â”‚       â”œâ”€â”€ replacetest.py      # Replacement testing
â”‚       â”œâ”€â”€ smoke_results.json  # Smoke test results
â”‚       â””â”€â”€ test.py             # Test runner
â”œâ”€â”€ prismatic/                  # OpenVLA codebase (original)
â””â”€â”€ vla-scripts/                # Model testing
    â”œâ”€â”€ deploy.py               # Model deployment script
    â”œâ”€â”€ finetune.py             # Fine-tuning script
    â”œâ”€â”€ train.py                # Training script
    â””â”€â”€ extern/                 # External conversion utilities
        â”œâ”€â”€ convert_openvla_weights_to_hf.py  # Weight conversion
        â”œâ”€â”€ test_openvla.py                   # OpenVLA testing
        â””â”€â”€ verify_openvla.py                 # OpenVLA verification
```

## ğŸ”§ Core Components

### ğŸ¯ Main Execution (`libero_run/`)
- **`main_agent_clean.py`**: Main execution script containing all tool module calls and agent logic implementation
- **`vision_planner_service.py`**: Service server for planner, Vision, and Language modules. Due to library version compatibility issues, we run the execution and verification module code in a separate process, communicating with the main process through socket communication. For module naming and content details, please refer to the paper.
- **`qwenvl.py`**: Wrapper function for the verification module

### ğŸ› ï¸ Tool Scripts (`script/`)
- **Computer Vision**: `SAM2_1.py`, `segvideo.py`, `mmgdino.py` - Advanced vision processing
- **Language Models**: `qwenvl_meg.py`, `Judge_simple.py` - Language understanding and judgment
- **Planning**: `SAPdivision.py` - Sub-action planning and task decomposition
- **Utilities**: `auto_DL.py`, `Wholebody.py` - Automation and analysis tools

### ğŸ—ï¸ Architecture (`prismatic/`)
The remaining code in the experiments folder is based on OpenVLA codebase
- **Backbone Models**: Support for various LLM and vision architectures
- **VLA Integration**: Specialized vision-language-action model implementations
- **Training Infrastructure**: Distributed training with DDP/FSDP support
- **Data Processing**: RLDS dataset integration and preprocessing

## ğŸ“Š Evaluation Scripts (`val_zsh/`)
- Comprehensive test scenarios covering different aspects of robot manipulation
- Goal-oriented tasks, object manipulation, and spatial reasoning evaluations

## ğŸ–ï¸ Citation & References
- **OpenVLA**: Open Vision-Language Agents (https://arxiv.org/abs/2304.09103)
- **Agentic-Robot**: Referenced codebase (https://github.com/Agentic-Robot/agentic-robot)

## ğŸš€ Deployment
- coming soon

## ğŸ”§ todo:
- Updating, new features coming soon.
